# 相关资源

- [视频](https://www.bilibili.com/video/BV1ha4y1H7sx)





# robots.txt协议

> 君子协议



例如：https://www.baidu.com/robots.txt



# HTTP与HTTPS协议

## HTTP协议

常用的请求头信息：

- User-Agent：请求载体的身份标识

- Connection：请求完毕后，是断开连接或保持连接



常用的响应体信息：

- Content-Type：服务器响应的信息的类型



## HTTPS协议

HTTPS协议：安全的超文本传输协议



加密方式：

- 对称密钥加密

    ![image-20220325225045555](%E7%88%AC%E8%99%AB.assets/image-20220325225045555.png)

    同时将密文和密钥发送给服务器

- 非对称密钥加密

    服务器制定加密方式，客户端根据此加密方式对内容进行加密。只对密文进行发送

    - 公钥：服务器制定的加密方式
    - 私钥：服务器制定的相应解密方式

    存在的问题：效率低；公钥可能被拦截及篡改

- 证书密钥加密（==HTTPS协议采用==）

    1. 服务器端制作公钥

    2. 将公钥提交给证书认证机构，并给公钥签名

    3. 证书认证结构将公钥与证书发送给客户端

        > 证书的数字签名是较难伪造的

    4. 客户端根据公钥对内容进行加密

    5. 客户端发送密文给服务器端

    6. 服务器根据私钥进行解密

    



# requests模块

此模块的作用：

- 模拟浏览器发送请求



响应对象response的方法：

- .text()：获取文本格式(字符串)的数据
- .json()：将其解析为json对象
- .content()：获取二进制格式的数据





## 基本使用

1. 指定URL
2. 发送请求（GET/POST）
3. 获取响应数据
4. 解析

```py
import requests

if __name__ == '__main__':
    url = "https://www.sogou.com/"
    # get方法会返回响应
    response = requests.get(url=url)

    page_text = response.text
    print(page_text)

    with open('./sogou.html', 'w', encoding='utf-8') as fp:
        fp.write(page_text)

    print("over")
```



## 使用UA伪装及用字典封装请求参数

```py
import requests

if __name__ == '__main__':
    url = "https://www.sogou.com/web"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0"
    }

    kw = input("enter a word:")
    # 将url参数封装到字典
    param = {
        "query": kw
    }

    # 使用UA伪装
    response = requests.get(url=url, params=param, headers=headers)
    page_text = response.text

    file_name = kw + ".html"
    with open("./" + file_name, 'w', encoding='utf-8') as fp:
        fp.write(page_text)

```



## 案例：百度翻译（Ajax请求与JSON）

```py
import json
import requests

if __name__ == "__main__":
    url = "https://fanyi.baidu.com/sug"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0"
    }

    keyword = input("输入:")

    data = {
        "kw": keyword
    }

    response = requests.post(url=url, data=data, headers=headers)
    json_obj = response.json()  # 仅在响应数据为json数据格式

    fp = open("./" + keyword + ".json", "w", encoding="utf-8")
    json.dump(json_obj, fp=fp, ensure_ascii=False)  # 中文不能用ASCII编码

```





# 数据解析

> 先爬取整张页面，再对目标内容进行解析

基本原理：

1. 定位标签
2. 提取标签属性或文本值



## 正则表达式

https://www.bilibili.com/video/BV1ha4y1H7sx?p=18&spm_id_from=pageDriver 11分钟

![image-20220326010704396](%E7%88%AC%E8%99%AB.assets/image-20220326010704396.png)



## bs4

> bs4是python独有的解析方式



环境：

- pip install bs4
- pip install lxml





bs4解析的原理：

1. 实例化一个BeautifulSoup对象，并将页面源码的数据加载到该对象中

    ```py
    from bs4 import BeautifulSoup
    
    if __name__ == "__main__":
        fp = open('./test.html','r',encoding='utf-8')
        soup = BeautifulSoup(fp,'lxml') #使用本地文档实例化BeautifulSoup对象
        
        page_text = response.text #或.content
        soup = BeautifulSoup(page_text,'lxml') #使用字符串实例化BeautifulSoup对象
    ```

    >[.text和.content的区别](https://blog.csdn.net/qq_38900441/article/details/79946377)

2. 调用BeautifulSoup对象的属性或方法，进行标签定位与数据提取



###BeautifulSoup对象

- 对象.标签名：返回==第一次出现==的相应标签

    ```py
    print(soup.a) #将打印第一次出现的a标签
    ```

- 对象.find()：

    - 返回==第一次出现==相应标签，与上面方式等同

        ```py
        print(soup.find("div"))
        ```

    - 结合属性定位：

        ```py
        print(soup.find("div",class_="btn"))#返回class属性有btn的第一个div
        ```

- 对象.find_all()：类似于find()方法，但返回==列表==

- 对象.select()：根据选择器进行选择，返回==列表==

    ```py
    soup.select(".btn") #根据类选择器.btn进行选择
    
    soup.select(".container > ul > li > a")#层级选择器
    soup.select(".container > ul > li  a")#层级选择器
    ```

    >- `>`表示一个层级
    >- `  `（空格）表示多个层级

- 获取标签内的文本数据

    ```py
    soup.a.text			#获取标签内的所有内容，包括内部的标签
    soup.a.string		#仅获取此标签的文本内容
    soup.a.get_text()	#获取标签内的所有内容，包括内部的标签
    ```

- 获取标签的属性值

    ```py
    soup.a.['href']
    ```

    

### 案例：爬取诗词名句网

- 

[网址](https://www.shicimingju.com/book/sanguoyanyi.html)

```py
import requests
from bs4 import BeautifulSoup

if __name__ == '__main__':
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0"
    }

    url = "https://www.shicimingju.com/book/sanguoyanyi.html"
    #若使用.text会乱码
    page_text = requests.get(url=url, headers=headers).content.decode('utf-8')
    soup = BeautifulSoup(page_text, "lxml")

    li_list = soup.select(".book-mulu > ul > li")

    fp = open("./sanguoyanyi.txt","w",encoding="utf-8")
    for li in li_list:
        title = li.a.string
        detail_url = "https://www.shicimingju.com/" + li.a["href"]

        detail_page_text = requests.get(url=detail_url, headers=headers).content.decode('utf-8')
        detail_soup = BeautifulSoup(detail_page_text, "lxml")
        div_content = detail_soup.find("div", class_="chapter_content")
        content = div_content.text

        fp.write(title+":"+content+"\n\n")
        print(title+"ok")

```

>[.text和.content的区别](https://blog.csdn.net/qq_38900441/article/details/79946377)



## XPath

https://www.bilibili.com/video/BV1ha4y1H7sx?p=23&spm_id_from=pageDriver