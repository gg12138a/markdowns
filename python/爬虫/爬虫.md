# 相关资源

- [视频](https://www.bilibili.com/video/BV1ha4y1H7sx)





# robots.txt协议

> 君子协议



例如：https://www.baidu.com/robots.txt



# HTTP与HTTPS协议

## HTTP协议

常用的请求头信息：

- User-Agent：请求载体的身份标识

- Connection：请求完毕后，是断开连接或保持连接



常用的响应体信息：

- Content-Type：服务器响应的信息的类型



## HTTPS协议

HTTPS协议：安全的超文本传输协议



加密方式：

- 对称密钥加密

    ![image-20220325225045555](%E7%88%AC%E8%99%AB.assets/image-20220325225045555.png)

    同时将密文和密钥发送给服务器

- 非对称密钥加密

    服务器制定加密方式，客户端根据此加密方式对内容进行加密。只对密文进行发送

    - 公钥：服务器制定的加密方式
    - 私钥：服务器制定的相应解密方式

    存在的问题：效率低；公钥可能被拦截及篡改

- 证书密钥加密（==HTTPS协议采用==）

    1. 服务器端制作公钥

    2. 将公钥提交给证书认证机构，并给公钥签名

    3. 证书认证结构将公钥与证书发送给客户端

        > 证书的数字签名是较难伪造的

    4. 客户端根据公钥对内容进行加密

    5. 客户端发送密文给服务器端

    6. 服务器根据私钥进行解密

    



# requests模块

此模块的作用：

- 模拟浏览器发送请求



响应对象response的方法：

- .text()：获取文本格式(字符串)的数据
- .json()：将其解析为json对象
- .content()：获取二进制格式的数据





## 基本使用

1. 指定URL
2. 发送请求（GET/POST）
3. 获取响应数据
4. 解析

```py
import requests

if __name__ == '__main__':
    url = "https://www.sogou.com/"
    # get方法会返回响应
    response = requests.get(url=url)

    page_text = response.text
    print(page_text)

    with open('./sogou.html', 'w', encoding='utf-8') as fp:
        fp.write(page_text)

    print("over")
```



## 使用UA伪装及用字典封装请求参数

```py
import requests

if __name__ == '__main__':
    url = "https://www.sogou.com/web"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0"
    }

    kw = input("enter a word:")
    # 将url参数封装到字典
    param = {
        "query": kw
    }

    # 使用UA伪装
    response = requests.get(url=url, params=param, headers=headers)
    page_text = response.text

    file_name = kw + ".html"
    with open("./" + file_name, 'w', encoding='utf-8') as fp:
        fp.write(page_text)

```



## 案例：百度翻译（Ajax请求与JSON）

```py
import json
import requests

if __name__ == "__main__":
    url = "https://fanyi.baidu.com/sug"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0"
    }

    keyword = input("输入:")

    data = {
        "kw": keyword
    }

    response = requests.post(url=url, data=data, headers=headers)
    json_obj = response.json()  # 仅在响应数据为json数据格式

    fp = open("./" + keyword + ".json", "w", encoding="utf-8")
    json.dump(json_obj, fp=fp, ensure_ascii=False)  # 中文不能用ASCII编码

```



## 验证码识别

使用==第三方自动识别==

- 超级鹰
- 云打码
- 图鉴打码

https://www.bilibili.com/video/BV1ha4y1H7sx?p=31&spm_id_from=pageDriver

## 模拟登录

- 登录人人网案例：https://www.bilibili.com/video/BV1ha4y1H7sx?p=33

- 模拟Cookie登录：https://www.bilibili.com/video/BV1ha4y1H7sx?p=36&spm_id_from=pageDriver 15：09

    ```py
    session = requests.session()
    response = session.get(url=url,headers=headers)
    ```

    







# 数据解析

> 先爬取整张页面，再对目标内容进行解析

基本原理：

1. 定位标签
2. 提取标签属性或文本值



## 正则表达式

https://www.bilibili.com/video/BV1ha4y1H7sx?p=18&spm_id_from=pageDriver 11分钟

![image-20220326010704396](%E7%88%AC%E8%99%AB.assets/image-20220326010704396.png)



## bs4

> bs4是python独有的解析方式



环境：

- pip install bs4
- pip install lxml





bs4解析的原理：

1. 实例化一个BeautifulSoup对象，并将页面源码的数据加载到该对象中

    ```py
    from bs4 import BeautifulSoup
    
    if __name__ == "__main__":
        fp = open('./test.html','r',encoding='utf-8')
        soup = BeautifulSoup(fp,'lxml') #使用本地文档实例化BeautifulSoup对象
        
        page_text = response.text #或.content
        soup = BeautifulSoup(page_text,'lxml') #使用字符串实例化BeautifulSoup对象
    ```

    >[.text和.content的区别](https://blog.csdn.net/qq_38900441/article/details/79946377)

2. 调用BeautifulSoup对象的属性或方法，进行标签定位与数据提取



###BeautifulSoup对象

- 对象.标签名：返回==第一次出现==的相应标签

    ```py
    print(soup.a) #将打印第一次出现的a标签
    ```

- 对象.find()：

    - 返回==第一次出现==相应标签，与上面方式等同

        ```py
        print(soup.find("div"))
        ```

    - 结合属性定位：

        ```py
        print(soup.find("div",class_="btn"))#返回class属性有btn的第一个div
        ```

- 对象.find_all()：类似于find()方法，但返回==列表==

- 对象.select()：根据选择器进行选择，返回==列表==

    ```py
    soup.select(".btn") #根据类选择器.btn进行选择
    
    soup.select(".container > ul > li > a")#层级选择器
    soup.select(".container > ul > li  a")#层级选择器
    ```

    >- `>`表示一个层级
    >- `  `（空格）表示多个层级

- 获取标签内的文本数据

    ```py
    soup.a.text			#获取标签内的所有内容，包括内部的标签
    soup.a.string		#仅获取此标签的文本内容
    soup.a.get_text()	#获取标签内的所有内容，包括内部的标签
    ```

- 获取标签的属性值

    ```py
    soup.a.['href']
    ```

    

### 案例：爬取诗词名句网

[网址](https://www.shicimingju.com/book/sanguoyanyi.html)

```py
import requests
from bs4 import BeautifulSoup

if __name__ == '__main__':
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0"
    }

    url = "https://www.shicimingju.com/book/sanguoyanyi.html"
    #若使用.text会乱码
    page_text = requests.get(url=url, headers=headers).content.decode('utf-8')
    soup = BeautifulSoup(page_text, "lxml")

    li_list = soup.select(".book-mulu > ul > li")

    fp = open("./sanguoyanyi.txt","w",encoding="utf-8")
    for li in li_list:
        title = li.a.string
        detail_url = "https://www.shicimingju.com/" + li.a["href"]

        detail_page_text = requests.get(url=detail_url, headers=headers).content.decode('utf-8')
        detail_soup = BeautifulSoup(detail_page_text, "lxml")
        div_content = detail_soup.find("div", class_="chapter_content")
        content = div_content.text

        fp.write(title+":"+content+"\n\n")
        print(title+"ok")

```

>[.text和.content的区别](https://blog.csdn.net/qq_38900441/article/details/79946377)



## XPath

https://www.bilibili.com/video/BV1ha4y1H7sx?p=23&spm_id_from=pageDriver



环境：

- pip install lxml

    

Xpath解析原理：

1. 利用html页面源码数据，实例化一个etree对象

    ```py
    from lxml import etree
    etree.parse(filepath) #利用本地文档实例化对象
    
    etree.HTML("page_text") #利用互联网获取的html源码数据
    ```

2. 定位标签和内容捕获（xpath方法+xpath表达式）



### xpath表达式

> 根据标签的层级关系进行定位



- etree对象.xpath()：得到的是==Element对象的列表==

    ```py
    from lxml import etree
    
    if __name__ == "__main__":
        parser = etree.HTMLParser(encoding="utf-8")
        tree = etree.parse("test.html", parser=parser)
    
        r = tree.xpath("/html/head/title")
        print(r)
    ```





表达式：

- `/`表示从根节点开始定位，或表示一个层级

    ```py
    r = tree.xpath("/html/head/title")
    ```

- `//`表示多个层级

    ```py
    r = tree.xpath("/html//title")
    ```

- `[@attrName='attrValue']`：根据属性定位

    ```py
    r = tree.xpath("//div[@class='container']") #根据class属性定位
    ```

- `[index]`：根据索引定位，此处索引从1开始

    ```py
    r = tree.xpath("//div[@class='container']/p[3]") #获取第三个p标签
    ```

- `/text()`：获取文本(仅获得直系标签的内容)

    ```py
    r = tree.xpath("//div/p/text()")
    ```

- `//text()`：获取文本（获得所有子孙标签的内容）

    ```py
    r = tree.xpath("//div/p//text()")
    ```

- `/@attrName`：获取属性值

    ```py
    r = tree.xpath("//div/img/@src") #获取img标签的src属性
    ```

    

### 案例：58二手房

https://www.bilibili.com/video/BV1ha4y1H7sx?p=25



### 案例：彼岸图网

https://www.bilibili.com/video/BV1ha4y1H7sx?p=26

乱码问题：10:21

- 可能的解决方式：手动设定响应数据的编码格式

    ```py
    response = requests.get(url=url, headers=headers)
    response.encoding="utf-8"
    page_text=response.text
    ```

- 对特定的乱码内容进行编码

    ```py
    img_name.encode('iso-8859-1').decode('gbk')
    ```

    

### 案例：全国城市名

xpath表达式可以使用`|`：

```py
tree.xpath("//div[@class = 'bottom']/ul/li/a "
           "|" 
           "//div[@class='bottom']/ul/div/li/a")
```





# 代理

客户端请求->**代理服务器**->目标服务器

代理相关的网站：

- 快代理
- 西祠代理
- www.goubanjia.com
- http://www.xiongmaodaili.com/

 

代理IP的匿名度：

- 透明：服务器知道此次请求使用了代理，且知道请求的真实IP
- 匿名：服务器知道此次请求使用了代理，但不知道请求的真实IP
- 高匿名：服务器不知道此次请求使用了代理

```py
response = requests.get(url=url, headers=headers,proxies={
    "https":"ip和端口号"
})
```



# 异步爬虫

实现方式：

- 多进程、多线程（不建议）

- 进程池、线程池（建议方式）

    降低系统为创建进程和销毁进程的开销



原则：==线程池仅应处理阻塞且耗时的操作==





模拟同步方式：

```py
import time


def get_content(url):
    print("正在爬取：" + url)
    time.sleep(2)
    print("下载成功：" + url)


url_list = ["1", "2", "3", "4", "5"]
start_time = time.time()

for url in url_list:
    get_content(url)

end_time = time.time()
print("%d second" % (end_time - start_time)) # 10 second
```



线程池方式：

```py
import time
from multiprocessing.dummy import Pool


def get_content(url):
    print("正在爬取：" + url)
    time.sleep(2)
    print("下载成功：" + url)


url_list = ["1", "2", "3", "4", "5"]

start_time = time.time()
pool = Pool(4)  # 实例化一个线程池对象，有4个线程
pool.map(get_content, url_list)
end_time = time.time()
print("%d second" % (end_time - start_time))  # 4 second
```



## 案例：梨视频(线程池)

==数据的可能来源==：

- 直接展现在HTML页面
- 来自于AJAX请求
- 数据在js代码中，在获取html页面后，再通过js动态添加数据



```py
from multiprocessing.dummy import Pool

import requests
from lxml import etree

if __name__ == "__main__":
    url = "https://www.pearvideo.com/category_5"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0"
    }
    page_text = requests.get(url=url, headers=headers).text

    tree = etree.HTML(page_text)
    li_list = tree.xpath("//ul[@id='listvideoListUl']/li")

    movie_url_list = []
    for li in li_list:
        detail_url = "https://www.pearvideo.com/" + li.xpath("./div[@class='vervideo-bd']/a/@href")[0]
        video_name = li.xpath("./div[@class='vervideo-bd']/a/div[2]/text()")[0] + ".mp4"

        # 获取详情页
        detail_page_text = requests.get(url=detail_url, headers=headers).text
        detail_tree = etree.HTML(detail_page_text)
        movie_id = detail_tree.xpath("//*[@id='poster']/@data-cid")[0]

        uurl = 'https://www.pearvideo.com/videoStatus.jsp'
        params = {
            "contId": movie_id,
            "mrd": "0.7163482212257486",
        }
        uurl_headers = {
            'Host': 'www.pearvideo.com',
            'Referer': detail_url
        }

        uurl_json = requests.get(url=uurl, headers=uurl_headers, params=params).json()
        src_url = uurl_json["videoInfo"]["videos"]["srcUrl"]
        src_url = src_url.replace(src_url.split('/')[-1].split('-')[0], "cont-" + movie_id)
        movie_url_list.append((src_url, video_name))

    print(movie_url_list)


    def get_video_data(url_tuple):
        url = url_tuple[0]
        name = url_tuple[1]
        data = requests.get(url=url, headers=headers).content

        with open(name, "wb") as fp:
            fp.write(data)
        print(name + "下载成功")


    pool = Pool(4)
    pool.map(get_video_data, movie_url_list)
    pool.close()
    pool.join()

```





## 单线程+异步协程

协程相关：

- event_loop：事件循环，相当于一个无限循环。

    可以==将一些函数注册到此事件循环上==，当满足某些条件时，函数就会被循环执行

- coroutine：协程对象，可以将协程对象注册到事件循环中，它会被事件循环调用

    可以使用async关键字定义一个方法，这个方法在调用时不会立即执行，而是返回一个协程对象

- task：任务，它是对协程对象的进一步封装，包含了任务的各个状态

- future：代表将来执行或还没有执行的任务。与task没有本质区别

- async：定义一个协程

- await：用于挂起阻塞方法的执行



### task与future对象

- 不使用task：

    ```py
    import asyncio
    
    
    async def requset(url):
        print("正在请求：", url)
        print("请求成功：", url)
    
    
    # async修饰的函数，在调用后不会立即执行，而是返回协程对象
    c = requset("www.baidu.com")
    print("not running now")
    
    # 创建事件循环对象
    loop_event = asyncio.get_event_loop()
    
    print("it will run")
    # 将协程对象，注册到协程对象,并启动事件循环
    loop_event.run_until_complete(c)
    ```

- 使用task的方式：

    ```py
    import asyncio
    
    
    async def requset(url):
        print("正在请求：", url)
        print("请求成功：", url)
    
    
    # async修饰的函数，在调用后不会立即执行，而是返回协程对象
    c = requset("www.baidu.com")
    
    # 创建事件循环对象
    loop_event = asyncio.get_event_loop()
    
    # 创建task对象
    task_c = loop_event.create_task(c)
    print(task_c)
    # <Task pending name='Task-1' coro=<requset() running at C:/Users/G_xy/PycharmProjects/pythonProject/xiecheng/xiechenglianxi.py:4>>
    
    # 注册并启动task对象
    loop_event.run_until_complete(task_c)
    print(task_c)
    # <Task finished name='Task-1' coro=<requset() done, defined at C:/Users/G_xy/PycharmProjects/pythonProject/xiecheng/xiechenglianxi.py:4> result=None>
    ```

- 使用future的方式：

    ```py
    import asyncio
    
    
    async def requset(url):
        print("正在请求：", url)
        print("请求成功：", url)
    
    
    # async修饰的函数，在调用后不会立即执行，而是返回协程对象
    c = requset("www.baidu.com")
    
    # 创建事件循环对象
    loop_event = asyncio.get_event_loop()
    
    # 注册协程对象c，创建future对象
    future_c = asyncio.ensure_future(c)
    print(future_c)
    # <Task pending name='Task-1' coro=<requset() running at C:\Users\G_xy\PycharmProjects\pythonProject\xiecheng\xiechenglianxi.py:4>>
    
    # 注册并启动future
    loop_event.run_until_complete(future_c)
    print(future_c)
    # <Task finished name='Task-1' coro=<requset() done, defined at C:\Users\G_xy\PycharmProjects\pythonProject\xiecheng\xiechenglianxi.py:4> result=None>
    ```

> task和future没有本质区别，只是两者的对象创建方式不一样



### 绑定回调

```py
import asyncio


async def requset(url):
    print("正在请求：", url)
    print("请求成功：", url)
    return url


def callback_func(task):
    print(task.result()) # 协程对象.result()方法将获得requset方法的返回值


# async修饰的函数，在调用后不会立即执行，而是返回协程对象
c = requset("www.baidu.com")

# 创建事件循环对象
loop_event = asyncio.get_event_loop()

# 使用协程对象,创建future对象
future_c = asyncio.ensure_future(c)

# 绑定回调函数
future_c.add_done_callback(callback_func)

# 注册并启动future
loop_event.run_until_complete(future_c)
```

> 回调函数callback_func在被调用时，将自动传入requset函数的返回值。
>
> 使用协程对象.result()方法，可以获得协程对象对应函数的返回值







### 多任务协程

https://www.bilibili.com/video/BV1ha4y1H7sx?p=46

```py
import asyncio
import time


async def request(url):
    print("开始下载：", url)
    # time.sleep(2)  # 若在异步协程(async方法)中，出现了同步模块相关的代码（如time.sleep()），就无法实现异步
    await asyncio.sleep(2)  # 若在asyncio中遇到阻塞操作，必须手动挂起
    print("结束下载：", url)


urls = [
    "www.baidu.com",
    "www.sogou.com",
    "www.bing.com"
]

task_list = []
for url in urls:
    c = request(url)
    task = asyncio.ensure_future(c)
    task_list.append(task)

start_time = time.time()

loop = asyncio.get_event_loop()
# 需要将任务列表注册到wait中
loop.run_until_complete(asyncio.wait(task_list))

end_time = time.time()

print(end_time - start_time)
```

**注意**，若在异步协程(async方法)中，出现了同步模块相关的代码（如time.sleep()），就无法实现异步。



# aiohttp模块实现异步协程

> request模块的请求，是基于同步的，无法实现多任务协程。
>
> 而aiohttp模块是基于异步网络请求的模块



环境安装：pip install aiohttp

flask服务的代码如下：

```py
from flask import Flask
import time

app = Flask(__name__)


@app.route('/a')
def index_a():
    time.sleep(2)
    return 'a'


@app.route('/b')
def index_b():
    time.sleep(2)
    return 'b'


@app.route('/c')
def index_c():
    time.sleep(2)
    return 'c'


if __name__ == '__main__':
    app.run(threaded=True)
```







## 基本用法

```py
import requests
import asyncio
import time
import aiohttp


async def get_page(url):
    async with aiohttp.ClientSession() as session:
        async with await session.get(url=url) as response:
            page_text = await response.text()  # 此Response对象需要用text()方法获取相应数据，而非text属性
            print(page_text)


urls = [
    'http://localhost:5000/a',
    'http://localhost:5000/b',
    'http://localhost:5000/c'
]

tasks = []

start = time.time()

for url in urls:
    c = get_page(url)
    task = asyncio.ensure_future(c)
    tasks.append(task)

loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))

end = time.time()
print(end - start)
```



注意：此Response对象中（==注意与requests模块的Response对象做区分==）：

- text()方法返回字符串形式的响应数据
- read()方法返回二进制形式的响应数据
- json()方法返回json对象

- get或post方法中：

    - 仍传入字典headers

    - get请求的参数仍为params

    - post请求的参数仍为data

    - 但是，代理IP使用参数proxy，并且传入是字符串而非字典

        proxy='http://ip:port'



# selenium模块

自动化的模拟浏览器动作

此模块的功能：

- 便捷的，获取网站中==动态加载的数据==
- 便捷的，实现模拟登录



环境安装：

- pip install selenium

- 浏览器的驱动程序：以chrome浏览器为例https://chromedriver.storage.googleapis.com/index.html

    驱动程序与浏览器的映射关系：



基本使用：

```py
from selenium import webdriver
from lxml import etree
from time import sleep

# 实例化浏览器对象
browser = webdriver.Chrome(executable_path='./chromedriver')

# 指定浏览器行为动作：

browser.get("http://scxk.nmpa.gov.cn:81/xk/")  # 发起请求
page_text = browser.page_source  # 获取当前页面的源码
tree = etree.HTML(page_text)
li_list = tree.xpath('//*[@id="gzlist"]/li')
for li in li_list:
    title = li.xpath('./dl/@title')[0]
    detail_url = li.xpath('./dl/a/@href')[0]
    xukezheng = li.xpath('./ol/@title')[0]
    print(title, xukezheng, detail_url)

sleep(5)
browser.quit()
```



以淘宝首页为案例：

```py
import time

from selenium import webdriver

# 实例化浏览器对象
browser = webdriver.Chrome(executable_path='./chromedriver')

# 指定浏览器行为动作：

browser.get("https://www.taobao.com/")  # 发起请求

# 屏幕滚动js代码：window.scrollTo(0,document.body.scrollHeight)
browser.execute_script('window.scrollTo(0,document.body.scrollHeight)')
time.sleep(1)


search_input = browser.find_element_by_id('q')
search_input.send_keys('Iphone')

btn = browser.find_element_by_css_selector('.btn-search')
btn.click()

browser.get("https://www.baidu.com/")
browser.back()
time.sleep(1)
browser.forward()
```





## API

- 实例化浏览器对象

    ```py
    browser = webdriver.Chrome(executable_path='./chromedriver')
    ```

- 发起请求

    ```py
    browser.get("http://scxk.nmpa.gov.cn:81/xk/")  # 发起请求
    ```

- 获取当前页面的HTML源码

    ```py
    page_text = browser.page_source  # 获取当前页面的源码
    ```

- 标签定位：find开头的一系列方法

    ```
    search_input = browser.find_element_by_id('id')
    ```

- 标签交互：

    ```py
    search_input.send_keys('输入的内容')
    
    btn.click()
    ```

- 浏览器控制：

    ```py
    browser.execute_script('window.scrollTo(0,document.body.scrollHeight)')
    browser.back()
    browser.forward()
    browser.quit()
    ```

    

    

## selenium处理iframe与动作链

> 被包含在iframe标签内的元素，不能直接获取

```py
import time

from selenium import webdriver
from selenium.webdriver import ActionChains  # 导入动作链对应的类

# 实例化浏览器对象
browser = webdriver.Chrome(executable_path='./chromedriver')

# 指定浏览器行为动作：

url = 'https://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'
browser.get(url=url)

browser.switch_to.frame('iframeResult')  # 切换浏览器标签定位的作用域
div = browser.find_element_by_id('draggable')

action_obj = ActionChains(browser)
action_obj.click_and_hold(div)  # 点击并长按此div标签

for i in range(5):
    action_obj.move_by_offset(25, 0).perform()  # perform表示立即执行动作链
    time.sleep(0.3)

action_obj.release()


browser.switch_to.default_content()
logo = browser.find_element_by_xpath('/html/body/header/div/div[1]')
print(logo)

time.sleep(3)
browser.quit()

```



动作链：

```py
from selenium.webdriver import ActionChains  # 导入动作链对应的类

action = ActionChains(browser) #实例化动作链对象

action.click_and_hold(x,y) #点击并长按
action.move_by_offset(x,y) 

action.perform() #让动作链立刻执行

action.release() #释放动作链
```





## 无头浏览器与规避检测

>无头浏览器：后台执行



无头浏览器：

```py
from selenium import webdriver
from selenium.webdriver.chrome.options import Options


chrome_options = Options()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--disable-gpu')
browser = webdriver.Chrome(executable_path='./chromedriver',chrome_options=chrome_options)

browser.get("https://www.baidu.com/")  
page_text = browser.page_source
print(page_text)

browser.quit()
```



规避检测：

```py
from selenium import webdriver
from selenium.webdriver import ChromeOptions

option = ChromeOptions()
option.add_experimental_option('excludeSwitches', ['enable-automation'])
browser = webdriver.Chrome(executable_path='./chromedriver', options=option)

```



## 案例：12306模拟登录

https://www.bilibili.com/video/BV1ha4y1H7sx?p=56&spm_id_from=pageDriver

关于验证码的解决方案：

- 截屏9：55
- 确定区域12：46
- 图片裁剪：21：34

```py
browser.save_screenshot()
```



# scrapy框架

scrapy框架提供的功能：

- 高性能的持久化存储
- 异步的数据下载
- 高性能的数据解析
- 分布式



[环境的安装(windows)](https://www.bilibili.com/video/BV1ha4y1H7sx?p=59&spm_id_from=pageDriver)： 

1. pip install wheel
2. 下载twisted：http://www.lfd.uci.edu/~gohklke/pythonlibs/#twisted
3. 安装twisted：pip install Twisted-17.1.0-cp36-cp36m-win_amd64.whl
4. pip install pywin32
5. pip install scrapy

在终端中输入scrapy指令，没有报错即为成功

> python3.9：直接pip install scrapy



## 基本使用

1. 创建一个scrapy工程：

    ```sh
    scrapy startproject xxx(projectname)
    ```

2. 在spiders子目录中，创建一个爬虫文件（cd到项目父目录）：

    ```sh
    scrapy genspider spiderName url
    例如：
    scrapy genspider first www.xxx.com
    ```

    对应的py文件：

    ```py
    import scrapy
    
    
    class FirstSpider(scrapy.Spider):
        name = 'first' #爬虫的名称
    
        #允许的域名：用于限定start_url中的请求可以访问的域名，通常注释掉
        allowed_domains = ['www.xxx.com'] 
        
        #起始的url：该列表中的url，会被scrapy自动请求
        start_urls = ['http://www.xxx.com/'] 
    
        #用作数据解析：response参数即为请求的响应对象
        def parse(self, response):
            pass
    
    ```

3. 设置settings.py:

    - 设置ROBOTSTXT_OBEY：

        ```py
        ROBOTSTXT_OBEY = False
        ```

        否则在爬取诸如百度等网站时，将拒绝连接

        ![image-20220327002658580](%E7%88%AC%E8%99%AB.assets/image-20220327002658580.png)

    - 设置USER-AGENT：

4. 执行工程：

    ```sh
    scrapy crawl spiderName
    例如：
    scrapy crawl first
    ```





工程目录结构如下：

![image-20220327001020004](%E7%88%AC%E8%99%AB.assets/image-20220327001020004.png)





关于日志输出问题：

- scrapy crawl spiderName --nolog：完全不输出日志（包括错误提示）

- settings.py中配置：

    ```py
    LOG_LEVEL = 'ERROR'
    ```

    



## scrapy的数据解析

```py
import scrapy


class FirstSpider(scrapy.Spider):
    name = 'first'
    # allowed_domains = ['www.xxx.com']
    start_urls = ['https://www.baidu.com/', 'https://www.sogou.com']

    def parse(self, response):
        """
        不再需要封装使用html源码实例化etree对象
        但返回的是：Selector类型的对象
        """
        response.xpath('//div[@id="content]/div/a/h2/text()"')
```



Selector对象：

- .extract()方法：返回Selector对象的data属性

    > 由Selector对象组成的列表也可以调用此方法，将获得列表的data属性拼接后的列表



[关于get()，getall()，extract()，extratc_first()的区别](https://segmentfault.com/a/1190000018559454)



## 持久化存储

### 基于终端指令的持久化存储

将把parse方法的返回值，持久化到本地的文本文件中

```py
import scrapy


class FirstSpider(scrapy.Spider):
    name = 'first'
    # allowed_domains = ['www.xxx.com']
    start_urls = ['https://www.baidu.com/', 'https://www.sogou.com']

    def parse(self, response):
        """
        不再需要封装使用html源码实例化etree对象
        但返回的是：Selector类型的对象
        """
        content = response.xpath('//div[@id="content]/div/a/h2/text()"')
        return content
```

```sh
scrapy crawl spiderName -o xxx.csv
```

> 可以存储为：json,jsonlines,jl,csv,xml,marshal,pickle

- 优点：简洁高效
- 确定：只能存储为特定的文件格式



### 基于管道的持久化存储

编码流程：

1. parse方法进行数据解析

    ```py
    import scrapy
    
    from scrapy.quickStart.quickStart.items import QuickstartItem
    
    
    class FirstSpider(scrapy.Spider):
        name = 'first'
        # allowed_domains = ['www.xxx.com']
        start_urls = ['https://www.baidu.com/']
    
        def parse(self, response):
            content = "this is content"
            author = "nobody"
    
            item = QuickstartItem()
            item['content'] = content
            item['author'] = author
    
            yield item  # 将item提交给管道
    ```

2. 定义Item类：

    ```py
    import scrapy
    
    
    class QuickstartItem(scrapy.Item):
        author = scrapy.Field()
        content = scrapy.Field()
    ```

3. 将解析的结果数据，封装到Item类型的对象中

4. 在配置文件中开启管道settings.py：

    ```py
    ITEM_PIPELINES = {
       'quickStart.pipelines.QuickstartPipeline': 300,
        # 300为优先级，值越小，优先级越高
    }
    ```

    > item对象将提交给优先级最高的管道类，然后按优先级进行传递

5. 将Item类型的对象，提交到管道

6. 在管道类的process_item方法中，对item对象封装的数据进行持久化

    ```py
    class QuickstartPipeline(object):
        fp = None  # 定义fp属性
    
        # 重写父类方法:该方法只在开始爬虫时被调用一次
        def open_spider(self, spider):
            self.fp = open('./qiubai.txt', 'w', encoding='utf-8')
    
        #该方法仅在结束爬虫时被调用一次
        def close_spider(self,spider):
            self.fp.close()
    
        def process_item(self, item, spider):
            author = item['author']
            content = item['content']
            self.fp.write(author + ":" + content + ":" + author + "\n")
            return item
    ```

    

- 优点：能使用自定义的持久化存储方式



## 基于Spider的全站数据爬取

> 即将网站中某板块下的全部页码对应的页面数据进行爬取

```py
import scrapy
from scrapy import cmdline


class IndexSpider(scrapy.Spider):
    name = 'index'
    # allowed_domains = ['www.netbian.com']
    start_urls = ['http://www.netbian.com/2k/']

    url = 'http://www.netbian.com/2k/index_%d.htm'
    page_num = 2

    def parse(self, response):
        print("begin")

        li_list = response.xpath('//*[@id="main"]/div[3]/ul/li')
        print(li_list)
        for li in li_list:
            title = li.xpath('./a/@title').get()
            img_src = li.xpath('./a/img/@src').get()
            print(title)
            print(img_src)

        if self.page_num <= 10:
            new_url = format(self.url % self.page_num)
            self.page_num += 1

            # 手动发送请求
            yield scrapy.Request(url=new_url, callback=self.parse)


if __name__ == '__main__':
    cmdline.execute('scrapy crawl index'.split())
```



## 五大核心组件

![image-20220327024143353](%E7%88%AC%E8%99%AB.assets/image-20220327024143353.png)

![image-20220327024734816](%E7%88%AC%E8%99%AB.assets/image-20220327024734816.png)



## 请求传参

- 跨解析方法的局部变量传递：使用Request方法的meta参数传递一个字典

    ![image-20220327025837597](%E7%88%AC%E8%99%AB.assets/image-20220327025837597.png)



https://www.bilibili.com/video/BV1ha4y1H7sx?p=67



## 图片爬取

Scrapy已经封装好的管道类：ImagesPipeline：

只需要将img的src属性提交到管道，管道将自动发送请求，并进行持久化存储



图片懒加载策略：

- 不在可视化区域内的img标签，其有名为类似src2的属性；
- 在可视化区域内的img标签，该属性才变为src属性

```py
import scrapy
from scrapy import cmdline

from scrapytest.imgsPro.imgsPro.items import ImgsproItem


class ImgSpider(scrapy.Spider):
    name = 'img'
    # allowed_domains = ['www.xxx.com']
    start_urls = ['https://sc.chinaz.com/tupian/']

    def parse(self, response):
        div_list = response.xpath('//div[@id="container"]/div')
        for div in div_list:
            # 注意，是src2而非src，是为了应对图片懒加载策略
            src = 'http:'+div.xpath('./div/a/img/@src2').get()
            print(src)
            item = ImgsproItem()
            item['src'] = src
            yield item


if __name__ == '__main__':
    cmdline.execute('scrapy crawl img'.split())
```



```py
import scrapy


class ImgsproItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    src = scrapy.Field()
```



```py
from scrapy.pipelines.images import ImagesPipeline


class MyImagePipeline(ImagesPipeline):
    # 重写方法：根据图片src发送请求
    def get_media_requests(self, item, info):
        yield scrapy.Request(item['src'])

    # 重写方法：指定图片持久化的路径
    def file_path(self, request, response=None, info=None, *, item=None):
        imgName = request.url.split('/')[-1]
        return imgName  # 图片存储位置在settings.py中配置IMAGES_STORE = './imgs'

    # 重写方法
    def item_completed(self, results, item, info):
        return item  # 将item返回给下一个管道类
```



```py
LOG_LEVEL = 'WARNING'
MEDIA_ALLOW_REDIRECTS = True
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0'

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

IMAGES_STORE = './imgs'

ITEM_PIPELINES = {
    'imgsPro.pipelines.MyImagePipeline': 300,
}
```



## 中间件

![image-20220327040526826](%E7%88%AC%E8%99%AB.assets/image-20220327040526826.png)

- 爬虫中间件

- 下载中间件

    - 位于引擎和下载器之间

    - 用于批量拦截整个工程中的请求和响应

    - 拦截请求：

        - UA伪装
            >配置文件中的UA伪装，是应用于全局的

        - 代理IP

    - 拦截响应：

        - 篡改响应数据



1. 编写下载中间件

    ```py
    class MiddleproDownloaderMiddleware:
        # Not all methods need to be defined. If a method is not defined,
        # scrapy acts as if the downloader middleware does not modify the
        # passed objects.
    
        # 用于拦截请求
        def process_request(self, request, spider):
            # Called for each request that goes through the downloader
            # middleware.
    
            # Must either:
            # - return None: continue processing this request
            # - or return a Response object
            # - or return a Request object
            # - or raise IgnoreRequest: process_exception() methods of
            #   installed downloader middleware will be called
            
            request.headers['User-Agent'] = ''  # 应从UA池子中获取
            return None
    
        # 用于拦截响应
        def process_response(self, request, response, spider):
            # Called with the response returned from the downloader.
    
            # Must either;
            # - return a Response object
            # - return a Request object
            # - or raise IgnoreRequest
            return response
    
        # 拦截发送异常的请求
        def process_exception(self, request, exception, spider):
            # 设置代理
            request.meta['proxy'] = 'http://ip:port'
            return request
    
    ```

2. 配置settings.py

    ```py
    DOWNLOADER_MIDDLEWARES = {
       'middlePro.middlewares.MiddleproDownloaderMiddleware': 543,
    }
    ```

    



拦截响应的[案例](https://www.bilibili.com/video/BV1ha4y1H7sx?p=74&spm_id_from=pageDriver)：网易新闻